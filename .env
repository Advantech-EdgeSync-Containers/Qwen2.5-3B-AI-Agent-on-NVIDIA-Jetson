# --- Model Settings ---
# Qwen2.5 3B model
MODEL_NAME=qwen2.5:3b

#Max prompt generation timeout to limit infinite run. (Default: 20)
MAX_PROMPT_GENERATION_TIMEOUT_IN_MIN=19

# --- Ollama Settings ---
COMPOSE_FILE_PATH=./docker-compose.yml

# --- Ollama Settings ---
# The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)
TEMPERATURE=0.5

# Sets the size of the context window used to generate the next token. (Default: 2048)
#NUM_CTX=4096

# The number of GPUs to use. On macOS it defaults to 1 to enable metal support, 0 to disable.
#NUM_GPU=

# Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance.
# It is recommended to set this value to the number of physical
# CPU cores your system has (as opposed to the logical number of cores).
#NUM_THREAD=

# System prompt (overrides what is defined in the Modelfile)
#SYSTEM=

# How long the model will stay loaded into memory.
# The parameter (Default: 5 minutes) can be set to:
#  1. a duration string in Golang (such as "10m" or "24h");
#  2. a number in seconds (such as 3600);
#  3. any negative number which will keep the model loaded in memory (e.g. -1 or "-1m");
#  4. 0 which will unload the model immediately after generating a response;
#  See the [Ollama documents](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately)"""
#KEEP_ALIVE=

# Sets how strongly to penalize repetitions. A higher value (e.g., 1.5)
# will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)
#REPEAT_PENALTY=

# Reduces the probability of generating nonsense. A higher value (e.g. 100)
# will give more diverse answers, while a lower value (e.g. 10)
# will be more conservative. (Default: 40)
TOP_K=50

# Works together with top-k. A higher value (e.g., 0.95) will lead
# to more diverse text, while a lower value (e.g., 0.5) will
# generate more focused and conservative text. (Default: 0.9)
TOP_P=0.5

# Full prompt or prompt template (overrides what is defined in the Modelfile)
#TEMPLATE=

#Other similar parameters can be added which are supported by Ollama, make sure to add them also in Ollama instantiation within llm_loader.py file

# --- LANGCHAIN FASTAPI Settings ---
FASTAPI_LANGCHAIN_AGENT_PORT=8000
OLLAMA_API_BASE=http://localhost:11434

# --- OPENWEBUI Settings ---
OPENWEBUI_PORT=3000
OPENAI_API_LANGCHAIN_BASE=http://localhost:8000

# Enables or disables chat title generation - default False
ENABLE_TITLE_GENERATION=False

# Enables or disables chat tag generation - default False
ENABLE_TAGS_GENERATION=False
